---
title: "Cron: Track Positions"
description: "Technical reference for the daily SERP position tracking job at /api/cron/track-positions."
---

The Track Positions cron job runs daily and submits SERP ranking tasks to DataForSEO for all tracked keywords across all active websites. Results are delivered asynchronously via the [SERP webhook](/reference/webhook-serp).

## Endpoint

```
GET /api/cron/track-positions
```

**Schedule:** Daily at 6:00 AM UTC (`0 6 * * *`)
**Max duration:** 300 seconds
**Auth:** `Authorization: Bearer {CRON_SECRET}`

## Who it affects

Only websites belonging to users with an **active or trialing subscription**, or users flagged as **VIP**. Within that set, only websites that have tracked keywords in the `tracked_keywords` table are processed.

## What it does

The job fetches all tracked keywords from the database and submits them to DataForSEO as asynchronous SERP tasks. Results are not returned inline — DataForSEO processes the tasks and delivers results to the SERP webhook when complete.

### Task submission

Keywords are submitted in batches of 100 to stay within DataForSEO's batch size limits. Each task is submitted with:

- The keyword to rank-check
- Location and language codes from the tracked keyword settings
- A `tag` containing `{ websiteId, domain }` — used by the webhook to route the result back to the correct website
- A `pingback` URL pointing to the SERP webhook endpoint

```
Pingback URL format:
https://rank.pagelyft.studio/api/webhooks/dataforseo?secret={DATAFORSEO_WEBHOOK_SECRET}&id=$id&tag=$tag
```

<Info>
  `$id` and `$tag` are DataForSEO template variables, not application variables. DataForSEO substitutes the actual task ID and tag values when it calls the pingback URL.
</Info>

## Execution flow

```
1. Authenticate request (verify CRON_SECRET)
2. Fetch active subscriber + VIP user IDs
3. Fetch all active (non-deleted) websites for those users
4. For each website:
   a. Fetch tracked keywords from tracked_keywords table
   b. If no tracked keywords → skip
   c. Batch keywords into groups of 100
   d. For each batch → submit SERP tasks to DataForSEO with pingback URL
5. Return 200 with summary
```

Results arrive asynchronously via the [SERP Webhook](/reference/webhook-serp) over the next several minutes to hours after submission.

## Database tables

| Table | Operation | Purpose |
|---|---|---|
| `subscriptions` | Read | Find users with active/trialing plans |
| `auth.users` | Read | Find VIP users |
| `websites` | Read | Get all active websites |
| `tracked_keywords` | Read | Get keywords to track for each website |

This job does not write to the database directly. Position data arrives via the webhook.

## External APIs

**DataForSEO SERP API** — `googleOrganicTaskPost` is called with batches of up to 100 keyword tasks. Each task requests the top 100 organic results for a keyword at the configured location and language.

## Error handling

Errors in one batch of SERP tasks do not abort the job. If a batch submission fails, the error is logged and the next batch proceeds. Websites with no tracked keywords are skipped silently.

The endpoint returns HTTP 200 even when individual tasks fail. Check Vercel function logs to diagnose failures.

<Warning>
  If this job times out before processing all keyword batches, some tracked keywords will not receive updated positions that day. The next day's run will pick them up. Minimize tracked keywords per website if timeouts occur frequently.
</Warning>

## Response format

```json
{
  "websites": 8,
  "results": [
    { "domain": "example.com", "tasks": 42 },
    { "domain": "another.com", "tasks": 15 },
    { "domain": "third.com", "tasks": 0, "reason": "no tracked keywords" }
  ]
}
```

## Manual trigger

```bash
curl https://rank.pagelyft.studio/api/cron/track-positions \
  -H "Authorization: Bearer $CRON_SECRET"
```

Use this after adding new tracked keywords or after a failed daily run.

## Supabase client used

This job uses the Supabase **service role client** (`SUPABASE_SERVICE_ROLE_KEY`) to bypass Row Level Security. It reads all websites and tracked keywords regardless of which user owns them.

<Warning>
  Do not expose this endpoint publicly or remove the auth check. A successful unauthenticated call would submit SERP tasks for every tracked keyword in the database, consuming DataForSEO API credits.
</Warning>

## Related

- [Cron Schedules](/reference/cron-schedules) — overview of all cron jobs
- [Cron: Refresh Websites](/reference/cron-refresh-websites) — weekly data refresh job
- [Cron: Run Scheduled Audits](/reference/cron-run-scheduled-audits) — daily site audit scheduling (previously part of this job)
- [Webhook: SERP](/reference/webhook-serp) — receives SERP task results
- [Environment Variables](/reference/environment-variables) — `CRON_SECRET`, `DATAFORSEO_*` configuration
